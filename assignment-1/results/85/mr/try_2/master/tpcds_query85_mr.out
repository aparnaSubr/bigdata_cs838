
Logging initialized using configuration in jar:file:/home/ubuntu/software/hive-1.2.1/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Hive history file=/tmp/ubuntu/hive_job_log_7ceb39c7-bb41-4adf-b2b7-0d2d8add9629_315861696.txt
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/ubuntu/software/tez-0.7.1-SNAPSHOT-minimal/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/software/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 0.988 seconds
Query ID = ubuntu_20161001110042_91ae3037-4af2-47e4-8190-d70c978010cd
Total jobs = 16
Stage-1 is selected by condition resolver.
Launching Job 1 out of 16
Number of reduce tasks not specified. Estimated from input data size: 32
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0165, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0165/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0165
Hadoop job information for Stage-1: number of mappers: 30; number of reducers: 32
2016-10-01 11:00:54,462 Stage-1 map = 0%,  reduce = 0%
2016-10-01 11:01:07,077 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 18.23 sec
2016-10-01 11:01:08,125 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 72.98 sec
2016-10-01 11:01:13,324 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 92.9 sec
2016-10-01 11:01:14,363 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 104.06 sec
2016-10-01 11:01:17,467 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 118.63 sec
2016-10-01 11:01:22,646 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 145.61 sec
2016-10-01 11:01:23,758 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 160.23 sec
2016-10-01 11:01:24,892 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 171.25 sec
2016-10-01 11:01:25,997 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 196.47 sec
2016-10-01 11:01:27,075 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 197.66 sec
2016-10-01 11:01:34,407 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 213.36 sec
2016-10-01 11:01:35,445 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 233.17 sec
2016-10-01 11:01:36,484 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 257.53 sec
2016-10-01 11:01:37,527 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 268.62 sec
2016-10-01 11:01:38,581 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 271.15 sec
2016-10-01 11:01:39,633 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 276.75 sec
2016-10-01 11:01:40,671 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 281.45 sec
2016-10-01 11:01:41,722 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 286.14 sec
2016-10-01 11:01:42,764 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 289.69 sec
2016-10-01 11:01:43,813 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 297.32 sec
2016-10-01 11:01:44,850 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 302.35 sec
2016-10-01 11:01:45,898 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 307.92 sec
2016-10-01 11:01:48,013 Stage-1 map = 84%,  reduce = 0%, Cumulative CPU 312.67 sec
2016-10-01 11:01:49,046 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 318.36 sec
2016-10-01 11:01:50,083 Stage-1 map = 87%,  reduce = 0%, Cumulative CPU 320.76 sec
2016-10-01 11:01:52,156 Stage-1 map = 91%,  reduce = 0%, Cumulative CPU 329.22 sec
2016-10-01 11:01:53,191 Stage-1 map = 92%,  reduce = 0%, Cumulative CPU 332.48 sec
2016-10-01 11:01:54,241 Stage-1 map = 98%,  reduce = 0%, Cumulative CPU 342.62 sec
2016-10-01 11:01:55,281 Stage-1 map = 99%,  reduce = 0%, Cumulative CPU 345.61 sec
2016-10-01 11:01:58,372 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 348.25 sec
2016-10-01 11:02:08,815 Stage-1 map = 100%,  reduce = 9%, Cumulative CPU 366.12 sec
2016-10-01 11:02:09,885 Stage-1 map = 100%,  reduce = 18%, Cumulative CPU 385.51 sec
2016-10-01 11:02:10,934 Stage-1 map = 100%,  reduce = 24%, Cumulative CPU 399.04 sec
2016-10-01 11:02:11,984 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 413.74 sec
2016-10-01 11:02:13,075 Stage-1 map = 100%,  reduce = 40%, Cumulative CPU 432.2 sec
2016-10-01 11:02:14,189 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 451.91 sec
2016-10-01 11:02:19,426 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 458.91 sec
2016-10-01 11:02:20,473 Stage-1 map = 100%,  reduce = 62%, Cumulative CPU 477.33 sec
2016-10-01 11:02:21,546 Stage-1 map = 100%,  reduce = 68%, Cumulative CPU 490.7 sec
2016-10-01 11:02:22,600 Stage-1 map = 100%,  reduce = 81%, Cumulative CPU 516.0 sec
2016-10-01 11:02:23,658 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 523.31 sec
2016-10-01 11:02:24,692 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 535.88 sec
2016-10-01 11:02:25,728 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 548.86 sec
2016-10-01 11:02:26,759 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 555.59 sec
MapReduce Total cumulative CPU time: 9 minutes 15 seconds 590 msec
Ended Job = job_1475271333482_0165
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/ubuntu/software/tez-0.7.1-SNAPSHOT-minimal/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/software/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Execution log at: /tmp/ubuntu/ubuntu_20161001110042_91ae3037-4af2-47e4-8190-d70c978010cd.log
2016-10-01 11:02:31	Starting to launch local task to process map join;	maximum memory = 932184064
2016-10-01 11:02:32	Dump the side-table for tag: 1 with group count: 876 into file: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10030/HashTable-Stage-30/MapJoin-mapfile91--.hashtable
2016-10-01 11:02:32	Uploaded 1 File to: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10030/HashTable-Stage-30/MapJoin-mapfile91--.hashtable (17481 bytes)
2016-10-01 11:02:32	End of local task; Time Taken: 1.474 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 16
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1475271333482_0166, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0166/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0166
Hadoop job information for Stage-30: number of mappers: 4; number of reducers: 0
2016-10-01 11:02:39,276 Stage-30 map = 0%,  reduce = 0%
2016-10-01 11:02:51,677 Stage-30 map = 38%,  reduce = 0%, Cumulative CPU 23.44 sec
2016-10-01 11:02:54,783 Stage-30 map = 100%,  reduce = 0%, Cumulative CPU 35.3 sec
MapReduce Total cumulative CPU time: 35 seconds 300 msec
Ended Job = job_1475271333482_0166
Stage-41 is filtered out by condition resolver.
Stage-42 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 16
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0167, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0167/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0167
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 2
2016-10-01 11:03:01,795 Stage-3 map = 0%,  reduce = 0%
2016-10-01 11:03:11,119 Stage-3 map = 20%,  reduce = 0%, Cumulative CPU 3.89 sec
2016-10-01 11:03:14,251 Stage-3 map = 53%,  reduce = 0%, Cumulative CPU 13.04 sec
2016-10-01 11:03:15,301 Stage-3 map = 83%,  reduce = 0%, Cumulative CPU 25.27 sec
2016-10-01 11:03:20,501 Stage-3 map = 87%,  reduce = 0%, Cumulative CPU 31.61 sec
2016-10-01 11:03:23,619 Stage-3 map = 90%,  reduce = 0%, Cumulative CPU 34.63 sec
2016-10-01 11:03:26,725 Stage-3 map = 93%,  reduce = 0%, Cumulative CPU 37.85 sec
2016-10-01 11:03:31,901 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 42.82 sec
2016-10-01 11:03:42,240 Stage-3 map = 100%,  reduce = 71%, Cumulative CPU 57.64 sec
2016-10-01 11:03:45,353 Stage-3 map = 100%,  reduce = 78%, Cumulative CPU 64.13 sec
2016-10-01 11:03:48,455 Stage-3 map = 100%,  reduce = 84%, Cumulative CPU 70.46 sec
2016-10-01 11:03:51,558 Stage-3 map = 100%,  reduce = 91%, Cumulative CPU 76.86 sec
2016-10-01 11:03:54,659 Stage-3 map = 100%,  reduce = 98%, Cumulative CPU 83.2 sec
2016-10-01 11:03:55,695 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 85.35 sec
MapReduce Total cumulative CPU time: 1 minutes 25 seconds 350 msec
Ended Job = job_1475271333482_0167
Stage-39 is filtered out by condition resolver.
Stage-40 is filtered out by condition resolver.
Stage-4 is selected by condition resolver.
Launching Job 4 out of 16
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0168, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0168/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0168
Hadoop job information for Stage-4: number of mappers: 6; number of reducers: 2
2016-10-01 11:04:01,532 Stage-4 map = 0%,  reduce = 0%
2016-10-01 11:04:09,802 Stage-4 map = 17%,  reduce = 0%, Cumulative CPU 4.81 sec
2016-10-01 11:04:10,836 Stage-4 map = 33%,  reduce = 0%, Cumulative CPU 9.86 sec
2016-10-01 11:04:12,909 Stage-4 map = 50%,  reduce = 0%, Cumulative CPU 13.82 sec
2016-10-01 11:04:13,946 Stage-4 map = 67%,  reduce = 0%, Cumulative CPU 24.82 sec
2016-10-01 11:04:20,156 Stage-4 map = 87%,  reduce = 0%, Cumulative CPU 43.49 sec
2016-10-01 11:04:23,261 Stage-4 map = 94%,  reduce = 0%, Cumulative CPU 49.9 sec
2016-10-01 11:04:24,301 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 50.11 sec
2016-10-01 11:04:34,637 Stage-4 map = 100%,  reduce = 35%, Cumulative CPU 57.78 sec
2016-10-01 11:04:35,683 Stage-4 map = 100%,  reduce = 70%, Cumulative CPU 65.29 sec
2016-10-01 11:04:37,753 Stage-4 map = 100%,  reduce = 76%, Cumulative CPU 68.56 sec
2016-10-01 11:04:40,850 Stage-4 map = 100%,  reduce = 83%, Cumulative CPU 78.19 sec
2016-10-01 11:04:43,950 Stage-4 map = 100%,  reduce = 89%, Cumulative CPU 84.63 sec
2016-10-01 11:04:47,054 Stage-4 map = 100%,  reduce = 95%, Cumulative CPU 91.16 sec
2016-10-01 11:04:49,121 Stage-4 map = 100%,  reduce = 98%, Cumulative CPU 93.8 sec
2016-10-01 11:04:50,156 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 96.38 sec
MapReduce Total cumulative CPU time: 1 minutes 36 seconds 380 msec
Ended Job = job_1475271333482_0168
Stage-37 is filtered out by condition resolver.
Stage-38 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.
Launching Job 5 out of 16
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0169, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0169/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0169
Hadoop job information for Stage-5: number of mappers: 3; number of reducers: 2
2016-10-01 11:04:55,966 Stage-5 map = 0%,  reduce = 0%
2016-10-01 11:05:06,287 Stage-5 map = 33%,  reduce = 0%, Cumulative CPU 4.62 sec
2016-10-01 11:05:13,505 Stage-5 map = 68%,  reduce = 0%, Cumulative CPU 30.45 sec
2016-10-01 11:05:16,599 Stage-5 map = 78%,  reduce = 0%, Cumulative CPU 36.91 sec
2016-10-01 11:05:19,699 Stage-5 map = 89%,  reduce = 0%, Cumulative CPU 43.06 sec
2016-10-01 11:05:20,742 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 43.6 sec
2016-10-01 11:05:31,068 Stage-5 map = 100%,  reduce = 36%, Cumulative CPU 51.38 sec
2016-10-01 11:05:32,105 Stage-5 map = 100%,  reduce = 72%, Cumulative CPU 59.03 sec
2016-10-01 11:05:34,171 Stage-5 map = 100%,  reduce = 75%, Cumulative CPU 62.31 sec
2016-10-01 11:05:35,204 Stage-5 map = 100%,  reduce = 78%, Cumulative CPU 65.54 sec
2016-10-01 11:05:37,268 Stage-5 map = 100%,  reduce = 84%, Cumulative CPU 72.01 sec
2016-10-01 11:05:40,363 Stage-5 map = 100%,  reduce = 91%, Cumulative CPU 78.63 sec
2016-10-01 11:05:43,456 Stage-5 map = 100%,  reduce = 97%, Cumulative CPU 85.04 sec
2016-10-01 11:05:44,495 Stage-5 map = 100%,  reduce = 98%, Cumulative CPU 86.15 sec
2016-10-01 11:05:45,536 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 88.5 sec
MapReduce Total cumulative CPU time: 1 minutes 28 seconds 500 msec
Ended Job = job_1475271333482_0169
Stage-35 is selected by condition resolver.
Stage-36 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/ubuntu/software/tez-0.7.1-SNAPSHOT-minimal/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/software/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Execution log at: /tmp/ubuntu/ubuntu_20161001110042_91ae3037-4af2-47e4-8190-d70c978010cd.log
2016-10-01 11:05:50	Starting to launch local task to process map join;	maximum memory = 932184064
2016-10-01 11:05:53	Dump the side-table for tag: 1 with group count: 365 into file: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10014/HashTable-Stage-18/MapJoin-mapfile11--.hashtable
2016-10-01 11:05:53	Uploaded 1 File to: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10014/HashTable-Stage-18/MapJoin-mapfile11--.hashtable (7963 bytes)
2016-10-01 11:05:53	End of local task; Time Taken: 2.394 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 16
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1475271333482_0170, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0170/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0170
Hadoop job information for Stage-18: number of mappers: 2; number of reducers: 0
2016-10-01 11:05:58,424 Stage-18 map = 0%,  reduce = 0%
2016-10-01 11:06:12,871 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 22.08 sec
MapReduce Total cumulative CPU time: 22 seconds 80 msec
Ended Job = job_1475271333482_0170
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/ubuntu/software/tez-0.7.1-SNAPSHOT-minimal/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/software/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Execution log at: /tmp/ubuntu/ubuntu_20161001110042_91ae3037-4af2-47e4-8190-d70c978010cd.log
2016-10-01 11:06:17	Starting to launch local task to process map join;	maximum memory = 932184064
2016-10-01 11:06:19	Dump the side-table for tag: 1 with group count: 39 into file: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10012/HashTable-Stage-8/MapJoin-mapfile01--.hashtable
2016-10-01 11:06:19	Uploaded 1 File to: file:/tmp/ubuntu/7ceb39c7-bb41-4adf-b2b7-0d2d8add9629/hive_2016-10-01_11-00-42_376_4463217668916230782-1/-local-10012/HashTable-Stage-8/MapJoin-mapfile01--.hashtable (1662 bytes)
2016-10-01 11:06:19	End of local task; Time Taken: 1.125 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 8 out of 16
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0171, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0171/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0171
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 1
2016-10-01 11:06:24,732 Stage-8 map = 0%,  reduce = 0%
2016-10-01 11:06:35,014 Stage-8 map = 33%,  reduce = 0%, Cumulative CPU 7.34 sec
2016-10-01 11:06:37,069 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 9.26 sec
2016-10-01 11:06:43,241 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 10.96 sec
MapReduce Total cumulative CPU time: 10 seconds 960 msec
Ended Job = job_1475271333482_0171
Launching Job 9 out of 16
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475271333482_0172, Tracking URL = http://vm1:8088/proxy/application_1475271333482_0172/
Kill Command = /home/ubuntu/software/hadoop-2.6.0/bin/hadoop job  -kill job_1475271333482_0172
Hadoop job information for Stage-9: number of mappers: 1; number of reducers: 1
2016-10-01 11:06:49,056 Stage-9 map = 0%,  reduce = 0%
2016-10-01 11:06:54,221 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2016-10-01 11:07:00,419 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 2.34 sec
MapReduce Total cumulative CPU time: 2 seconds 340 msec
Ended Job = job_1475271333482_0172
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 30  Reduce: 32   Cumulative CPU: 555.59 sec   HDFS Read: 8124263543 HDFS Write: 226967742 SUCCESS
Stage-Stage-30: Map: 4   Cumulative CPU: 35.79 sec   HDFS Read: 227005634 HDFS Write: 235481511 SUCCESS
Stage-Stage-3: Map: 5  Reduce: 2   Cumulative CPU: 85.35 sec   HDFS Read: 316193629 HDFS Write: 293829902 SUCCESS
Stage-Stage-4: Map: 6  Reduce: 2   Cumulative CPU: 96.38 sec   HDFS Read: 374559511 HDFS Write: 348836332 SUCCESS
Stage-Stage-5: Map: 3  Reduce: 2   Cumulative CPU: 88.5 sec   HDFS Read: 391387333 HDFS Write: 419920213 SUCCESS
Stage-Stage-18: Map: 2   Cumulative CPU: 23.12 sec   HDFS Read: 419946600 HDFS Write: 85556487 SUCCESS
Stage-Stage-8: Map: 1  Reduce: 1   Cumulative CPU: 10.96 sec   HDFS Read: 85590089 HDFS Write: 1858 SUCCESS
Stage-Stage-9: Map: 1  Reduce: 1   Cumulative CPU: 2.34 sec   HDFS Read: 7071 HDFS Write: 1783 SUCCESS
Total MapReduce CPU Time Spent: 14 minutes 58 seconds 30 msec
OK
Time taken: 379.161 seconds, Fetched: 32 row(s)
52.31user 5.26system 6:26.36elapsed 14%CPU (0avgtext+0avgdata 1199160maxresident)k
115664inputs+491440outputs (109major+195298minor)pagefaults 0swaps
